{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from opacus import PrivacyEngine\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from utils import preprocess_data, train_private_logreg, train_logreg, test_private_logreg\n",
    "import matplotlib.pyplot as plt\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "import shutup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(94192, 22) (8827, 22)\n"
     ]
    }
   ],
   "source": [
    "train_sources = ['chapman', 'cpsc', 'ptb', 'sph']\n",
    "\n",
    "train_ids = []\n",
    "for source in train_sources:\n",
    "    df = pd.read_csv(f'data/sources_clean_zoher/clean_all_{source}.csv')\n",
    "    for idx, row in df.iterrows():\n",
    "        train_ids.append(row['path'].split('/')[-1].split('.')[0].split('_')[0])\n",
    "\n",
    "test_source = 'g12ec'\n",
    "test_ids = []\n",
    "df = pd.read_csv(f'data/sources_clean_zoher/clean_all_{test_source}.csv')\n",
    "for idx, row in df.iterrows():\n",
    "    test_ids.append(row['path'].split('/')[-1].split('.')[0].split('_')[0])\n",
    "\n",
    "train_feats = pd.DataFrame()\n",
    "for source in ['ChapmanShaoxing_Ningbo', 'CPSC_CPSC-Extra', 'PTB_PTBXL', 'SPH']:\n",
    "    source_feats = pd.read_csv(f'data/{source}_feats.csv')\n",
    "    train_feats = pd.concat((train_feats, source_feats), ignore_index=True)\n",
    "train_feats = train_feats[train_feats['id'].isin(train_ids)]\n",
    "\n",
    "\n",
    "df_test = pd.read_csv('data/G12EC_feats.csv')\n",
    "test_feats = df_test[df_test['id'].isin(test_ids)]\n",
    "\n",
    "X_train, y_train = preprocess_data(train_feats)\n",
    "X_test, y_test = preprocess_data(test_feats)\n",
    "\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training/Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.4046, Epsilon: 99.99\r"
     ]
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "batch_size = 20000\n",
    "lr = 50\n",
    "models = {}\n",
    "for epsilon in [1, 10, 100]:\n",
    "    model = train_private_logreg(X_train, y_train, epsilon=epsilon,\n",
    "                                 num_epochs=num_epochs, batch_size=batch_size, lr=lr)\n",
    "    models[f'e_{epsilon}'] = model\n",
    "\n",
    "models['e_inf'] = train_logreg(X_train, y_train, num_epochs=num_epochs, batch_size=20000, lr=50)\n",
    "\n",
    "train_scores = {}\n",
    "test_scores = {}\n",
    "\n",
    "for k in models.keys():\n",
    "    train_scores[f'{k}'] = test_private_logreg(models[k], X_train, y_train)\n",
    "    test_scores[f'{k}'] = test_private_logreg(models[k], X_test, y_test)\n",
    "\n",
    "print(f'         train scores                                    test scores')\n",
    "for k in train_scores.keys():\n",
    "    key_str = (k + '  ')[:5]  # Pad/truncate key for alignment\n",
    "    print(f'{key_str}   {train_scores[k]}      {test_scores[k]}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 500, Loss: 0.1690, Epsilon: 100.00\r"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "\n",
    "# Define 10 fixed, reproducible seeds\n",
    "fixed_seeds = [42, 7, 123, 2023, 999, 314, 0, 88, 17, 555]\n",
    "\n",
    "num_epochs = 500\n",
    "batch_size = int(0.5*len(X_train))\n",
    "lr = 20\n",
    "epsilons = [1, 10, 100]\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for seed in fixed_seeds:\n",
    "    np.random.seed(seed)\n",
    "    # If you're using other libs like torch or random, set those seeds too\n",
    "    # random.seed(seed)\n",
    "    # torch.manual_seed(seed)\n",
    "\n",
    "    models = {}\n",
    "\n",
    "    for epsilon in epsilons:\n",
    "        model = train_private_logreg(X_train, y_train, epsilon=epsilon,\n",
    "                                     num_epochs=num_epochs, batch_size=batch_size, lr=lr)\n",
    "        models[f'e_{epsilon}'] = model\n",
    "\n",
    "    models['e_inf'] = train_logreg(X_train, y_train, num_epochs=num_epochs,\n",
    "                                   batch_size=batch_size, lr=lr)\n",
    "\n",
    "    train_scores = {}\n",
    "    test_scores = {}\n",
    "\n",
    "    for k in models:\n",
    "        train_scores[k] = test_private_logreg(models[k], X_train, y_train)\n",
    "        test_scores[k] = test_private_logreg(models[k], X_test, y_test)\n",
    "\n",
    "    all_results.append({\n",
    "        'seed': seed,\n",
    "        'train_scores': train_scores,\n",
    "        'test_scores': test_scores\n",
    "    })\n",
    "\n",
    "# Save to pickle\n",
    "with open('logreg_results.pkl', 'wb') as f:\n",
    "    pickle.dump(all_results, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'seed': 42,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7787, 'micro_auc': 0.9031},\n",
       "   'e_10': {'macro_auc': 0.7981, 'micro_auc': 0.9079},\n",
       "   'e_100': {'macro_auc': 0.7975, 'micro_auc': 0.908},\n",
       "   'e_inf': {'macro_auc': 0.7985, 'micro_auc': 0.9081}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.7092, 'micro_auc': 0.7958},\n",
       "   'e_10': {'macro_auc': 0.7192, 'micro_auc': 0.815},\n",
       "   'e_100': {'macro_auc': 0.718, 'micro_auc': 0.8114},\n",
       "   'e_inf': {'macro_auc': 0.7177, 'micro_auc': 0.8063}}},\n",
       " {'seed': 7,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7768, 'micro_auc': 0.903},\n",
       "   'e_10': {'macro_auc': 0.798, 'micro_auc': 0.908},\n",
       "   'e_100': {'macro_auc': 0.7982, 'micro_auc': 0.908},\n",
       "   'e_inf': {'macro_auc': 0.7987, 'micro_auc': 0.908}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.7032, 'micro_auc': 0.7533},\n",
       "   'e_10': {'macro_auc': 0.7185, 'micro_auc': 0.8154},\n",
       "   'e_100': {'macro_auc': 0.7182, 'micro_auc': 0.8111},\n",
       "   'e_inf': {'macro_auc': 0.7214, 'micro_auc': 0.8105}}},\n",
       " {'seed': 123,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7777, 'micro_auc': 0.9038},\n",
       "   'e_10': {'macro_auc': 0.799, 'micro_auc': 0.9083},\n",
       "   'e_100': {'macro_auc': 0.7981, 'micro_auc': 0.908},\n",
       "   'e_inf': {'macro_auc': 0.7985, 'micro_auc': 0.908}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.7043, 'micro_auc': 0.7448},\n",
       "   'e_10': {'macro_auc': 0.7198, 'micro_auc': 0.8136},\n",
       "   'e_100': {'macro_auc': 0.7203, 'micro_auc': 0.8151},\n",
       "   'e_inf': {'macro_auc': 0.719, 'micro_auc': 0.8097}}},\n",
       " {'seed': 2023,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7792, 'micro_auc': 0.9044},\n",
       "   'e_10': {'macro_auc': 0.7974, 'micro_auc': 0.9079},\n",
       "   'e_100': {'macro_auc': 0.7982, 'micro_auc': 0.908},\n",
       "   'e_inf': {'macro_auc': 0.7988, 'micro_auc': 0.9081}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.712, 'micro_auc': 0.8078},\n",
       "   'e_10': {'macro_auc': 0.7177, 'micro_auc': 0.8152},\n",
       "   'e_100': {'macro_auc': 0.7162, 'micro_auc': 0.8105},\n",
       "   'e_inf': {'macro_auc': 0.7206, 'micro_auc': 0.8092}}},\n",
       " {'seed': 999,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7888, 'micro_auc': 0.9054},\n",
       "   'e_10': {'macro_auc': 0.7988, 'micro_auc': 0.9082},\n",
       "   'e_100': {'macro_auc': 0.7984, 'micro_auc': 0.908},\n",
       "   'e_inf': {'macro_auc': 0.7982, 'micro_auc': 0.9081}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.7097, 'micro_auc': 0.7651},\n",
       "   'e_10': {'macro_auc': 0.72, 'micro_auc': 0.8102},\n",
       "   'e_100': {'macro_auc': 0.7189, 'micro_auc': 0.8099},\n",
       "   'e_inf': {'macro_auc': 0.7192, 'micro_auc': 0.8088}}},\n",
       " {'seed': 314,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7738, 'micro_auc': 0.9035},\n",
       "   'e_10': {'macro_auc': 0.7974, 'micro_auc': 0.9078},\n",
       "   'e_100': {'macro_auc': 0.7989, 'micro_auc': 0.908},\n",
       "   'e_inf': {'macro_auc': 0.7991, 'micro_auc': 0.9081}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.705, 'micro_auc': 0.761},\n",
       "   'e_10': {'macro_auc': 0.7202, 'micro_auc': 0.8093},\n",
       "   'e_100': {'macro_auc': 0.7195, 'micro_auc': 0.8141},\n",
       "   'e_inf': {'macro_auc': 0.719, 'micro_auc': 0.8079}}},\n",
       " {'seed': 0,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7753, 'micro_auc': 0.9037},\n",
       "   'e_10': {'macro_auc': 0.7981, 'micro_auc': 0.908},\n",
       "   'e_100': {'macro_auc': 0.7984, 'micro_auc': 0.9081},\n",
       "   'e_inf': {'macro_auc': 0.7984, 'micro_auc': 0.9081}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.7073, 'micro_auc': 0.7783},\n",
       "   'e_10': {'macro_auc': 0.7187, 'micro_auc': 0.8106},\n",
       "   'e_100': {'macro_auc': 0.7185, 'micro_auc': 0.8117},\n",
       "   'e_inf': {'macro_auc': 0.7191, 'micro_auc': 0.8103}}},\n",
       " {'seed': 88,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7734, 'micro_auc': 0.903},\n",
       "   'e_10': {'macro_auc': 0.7984, 'micro_auc': 0.908},\n",
       "   'e_100': {'macro_auc': 0.7981, 'micro_auc': 0.908},\n",
       "   'e_inf': {'macro_auc': 0.798, 'micro_auc': 0.908}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.7005, 'micro_auc': 0.7832},\n",
       "   'e_10': {'macro_auc': 0.7185, 'micro_auc': 0.8084},\n",
       "   'e_100': {'macro_auc': 0.7189, 'micro_auc': 0.8127},\n",
       "   'e_inf': {'macro_auc': 0.7192, 'micro_auc': 0.814}}},\n",
       " {'seed': 17,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7838, 'micro_auc': 0.9041},\n",
       "   'e_10': {'macro_auc': 0.798, 'micro_auc': 0.908},\n",
       "   'e_100': {'macro_auc': 0.799, 'micro_auc': 0.9082},\n",
       "   'e_inf': {'macro_auc': 0.7985, 'micro_auc': 0.9082}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.7157, 'micro_auc': 0.7674},\n",
       "   'e_10': {'macro_auc': 0.719, 'micro_auc': 0.8149},\n",
       "   'e_100': {'macro_auc': 0.7199, 'micro_auc': 0.8102},\n",
       "   'e_inf': {'macro_auc': 0.7189, 'micro_auc': 0.8079}}},\n",
       " {'seed': 555,\n",
       "  'train_scores': {'e_1': {'macro_auc': 0.7807, 'micro_auc': 0.9048},\n",
       "   'e_10': {'macro_auc': 0.7978, 'micro_auc': 0.9079},\n",
       "   'e_100': {'macro_auc': 0.7988, 'micro_auc': 0.9081},\n",
       "   'e_inf': {'macro_auc': 0.7983, 'micro_auc': 0.9081}},\n",
       "  'test_scores': {'e_1': {'macro_auc': 0.7119, 'micro_auc': 0.7767},\n",
       "   'e_10': {'macro_auc': 0.7215, 'micro_auc': 0.8066},\n",
       "   'e_100': {'macro_auc': 0.72, 'micro_auc': 0.8152},\n",
       "   'e_inf': {'macro_auc': 0.7197, 'micro_auc': 0.8125}}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47096"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'e_1': {'macro_auc': 0.7092, 'micro_auc': 0.7958}, 'e_10': {'macro_auc': 0.7192, 'micro_auc': 0.815}, 'e_100': {'macro_auc': 0.718, 'micro_auc': 0.8114}, 'e_inf': {'macro_auc': 0.7177, 'micro_auc': 0.8063}}\n",
      "{'e_1': {'macro_auc': 0.7032, 'micro_auc': 0.7533}, 'e_10': {'macro_auc': 0.7185, 'micro_auc': 0.8154}, 'e_100': {'macro_auc': 0.7182, 'micro_auc': 0.8111}, 'e_inf': {'macro_auc': 0.7214, 'micro_auc': 0.8105}}\n",
      "{'e_1': {'macro_auc': 0.7043, 'micro_auc': 0.7448}, 'e_10': {'macro_auc': 0.7198, 'micro_auc': 0.8136}, 'e_100': {'macro_auc': 0.7203, 'micro_auc': 0.8151}, 'e_inf': {'macro_auc': 0.719, 'micro_auc': 0.8097}}\n",
      "{'e_1': {'macro_auc': 0.712, 'micro_auc': 0.8078}, 'e_10': {'macro_auc': 0.7177, 'micro_auc': 0.8152}, 'e_100': {'macro_auc': 0.7162, 'micro_auc': 0.8105}, 'e_inf': {'macro_auc': 0.7206, 'micro_auc': 0.8092}}\n",
      "{'e_1': {'macro_auc': 0.7097, 'micro_auc': 0.7651}, 'e_10': {'macro_auc': 0.72, 'micro_auc': 0.8102}, 'e_100': {'macro_auc': 0.7189, 'micro_auc': 0.8099}, 'e_inf': {'macro_auc': 0.7192, 'micro_auc': 0.8088}}\n",
      "{'e_1': {'macro_auc': 0.705, 'micro_auc': 0.761}, 'e_10': {'macro_auc': 0.7202, 'micro_auc': 0.8093}, 'e_100': {'macro_auc': 0.7195, 'micro_auc': 0.8141}, 'e_inf': {'macro_auc': 0.719, 'micro_auc': 0.8079}}\n",
      "{'e_1': {'macro_auc': 0.7073, 'micro_auc': 0.7783}, 'e_10': {'macro_auc': 0.7187, 'micro_auc': 0.8106}, 'e_100': {'macro_auc': 0.7185, 'micro_auc': 0.8117}, 'e_inf': {'macro_auc': 0.7191, 'micro_auc': 0.8103}}\n",
      "{'e_1': {'macro_auc': 0.7005, 'micro_auc': 0.7832}, 'e_10': {'macro_auc': 0.7185, 'micro_auc': 0.8084}, 'e_100': {'macro_auc': 0.7189, 'micro_auc': 0.8127}, 'e_inf': {'macro_auc': 0.7192, 'micro_auc': 0.814}}\n",
      "{'e_1': {'macro_auc': 0.7157, 'micro_auc': 0.7674}, 'e_10': {'macro_auc': 0.719, 'micro_auc': 0.8149}, 'e_100': {'macro_auc': 0.7199, 'micro_auc': 0.8102}, 'e_inf': {'macro_auc': 0.7189, 'micro_auc': 0.8079}}\n",
      "{'e_1': {'macro_auc': 0.7119, 'micro_auc': 0.7767}, 'e_10': {'macro_auc': 0.7215, 'micro_auc': 0.8066}, 'e_100': {'macro_auc': 0.72, 'micro_auc': 0.8152}, 'e_inf': {'macro_auc': 0.7197, 'micro_auc': 0.8125}}\n"
     ]
    }
   ],
   "source": [
    "for i in all_results:\n",
    "    print(i['test_scores'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "47096"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "int(0.5*len(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = preprocess_data(test_size=0)\n",
    "param_grid = {\n",
    "    'num_epochs': [10, 20, 50, 100, 200, 500],\n",
    "    'batch_size': [0.05, 0.1, 0.2, 0.5],\n",
    "    'lr': [0.1, 1, 5, 10, 20, 50]\n",
    "}\n",
    "\n",
    "mskf = MultilabelStratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "scores = {}\n",
    "best_score = 0\n",
    "best_prms = None\n",
    "for num_epochs in param_grid['num_epochs']:\n",
    "    for batch_size in param_grid['batch_size']:\n",
    "        for lr in param_grid['lr']:\n",
    "            scores[f'{num_epochs}_{batch_size}_{lr}'] = []\n",
    "            for train_index, test_index in mskf.split(X, y):\n",
    "                X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "                y_train, y_test = y.iloc[train_index], y.iloc[test_index] \n",
    "                model = train_private_logreg(X_train, y_train, epsilon=1, num_epochs=num_epochs,\n",
    "                                             batch_size=int(batch_size*len(X_train)), lr=lr)\n",
    "                macro_auc = test_private_logreg(model, X_test, y_test)['macro_auc']\n",
    "                scores[f'{num_epochs}_{batch_size}_{lr}'].append(macro_auc)\n",
    "            # print(scores)\n",
    "            if np.mean(scores[f'{num_epochs}_{batch_size}_{lr}']) > best_score:\n",
    "                best_score = np.mean(scores[f'{num_epochs}_{batch_size}_{lr}'])\n",
    "                best_prms = {'num_epochs': num_epochs,\n",
    "                             'batch_size': batch_size,\n",
    "                             'lr': lr}\n",
    "                print(best_prms, best_score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
